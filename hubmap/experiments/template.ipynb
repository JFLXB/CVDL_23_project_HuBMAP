{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hubmap.experiments.load_data import make_expert_loader\n",
    "from hubmap.experiments.load_data import make_annotated_loader\n",
    "from hubmap.dataset import transforms as T\n",
    "from hubmap.losses import BCEDiceLoss\n",
    "from hubmap.metrics import IoU\n",
    "from hubmap.training import train\n",
    "from hubmap.training import LRScheduler\n",
    "from hubmap.training import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data that we need for the experiment. This includes the training data, the validation (test) data that we will use for training.\n",
    "\n",
    "For this, depending on the experiments we use different transformations on the data. The following transformations are a minimal example. Furhter transformations should be included for more sophisticated experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = T.Compose(\n",
    "    [T.ToTensor(), T.Resize((512, 512)), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "test_transformations = T.Compose(\n",
    "    [T.ToTensor(), T.Resize((512, 512)), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the experiment we may want to load all annotated images or just the ones that are annotated by experts.\n",
    "\n",
    "Here we create a function to load all the images that are annotated (not only the ones by experts).\n",
    "The created function can than be used to load the data loaders with a specific batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train, test split ratio is set to 0.8 by default.\n",
    "# Meaning 80% of the data is used for training and 20% for testing.\n",
    "load_annotated_data = make_annotated_loader(train_transformations, test_transformations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we determine the device we want to train on. \n",
    "If a GPU is available, we use it, otherwise we fall back to the CPU. \n",
    "We also set the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the model we want to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hubmap.models import DPT\n",
    "\n",
    "model = DPT(num_classes=3, features=128).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the other modules needed for training, such as the loss measure, and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = BCEDiceLoss()\n",
    "# WE ARE ONLY INTERESTED IN THE IoU OF THE BLOOD VESSEL CLASS FOR NOW.\n",
    "benchmark = IoU(class_index=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the trainer and start training. The trainer is responsible for running the training loop, saving checkpoints, and logging metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader, test_loader = load_annotated_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition we want to have a dynamic adjustment of the learning rate.\n",
    "lr_scheduler = LRScheduler(optimizer, patience=10, min_lr=1e-6, factor=0.5)\n",
    "# We will ignore the early stopping in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'lr_scheduler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m train(\n\u001b[1;32m      2\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m      5\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m      6\u001b[0m     train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m      7\u001b[0m     test_loader\u001b[39m=\u001b[39;49mtest_loader,\n\u001b[1;32m      8\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m      9\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[1;32m     10\u001b[0m     checkpoint_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdpt_trials\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     lr_scheduler\u001b[39m=\u001b[39;49mlr_scheduler,\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'lr_scheduler'"
     ]
    }
   ],
   "source": [
    "result = train(\n",
    "    num_epochs=2,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    benchmark=benchmark,\n",
    "    checkpoint_name=\"dpt_trials\",\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the results.\n",
    "(*this needs improvements + better and more visualizations for the final paper*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from hubmap.visualization import visualize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_path = Path().cwd() / \"figures\"\n",
    "os.makedirs(figures_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fig, benchmark_fig = visualize_result(result)\n",
    "loss_fig.savefig(Path(figures_path, \"dpt_loss.png\"))\n",
    "benchmark_fig.savefig(Path(figures_path, \"dpt_benchmark.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "J",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a58ea3f06b7c035f03696c8d262430e329e4f861a31188ab1909a2b4ddd27d3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
