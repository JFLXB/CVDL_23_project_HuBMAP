{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from hubmap.experiments.load_data import make_expert_loader\n",
    "from hubmap.experiments.load_data import make_annotated_loader\n",
    "from hubmap.dataset import transforms as T\n",
    "from hubmap.losses import BCEDiceLoss\n",
    "from hubmap.metrics import IoU\n",
    "from hubmap.training import train\n",
    "from hubmap.training import LRScheduler\n",
    "from hubmap.training import EarlyStopping\n",
    "\n",
    "from hubmap.metrics import model_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data that we need for the experiment. This includes the training data, the validation (test) data that we will use for training.\n",
    "\n",
    "For this, depending on the experiments we use different transformations on the data. The following transformations are a minimal example. Furhter transformations should be included for more sophisticated experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINIMUM SIZE OF IMAGES FOR MODEL INPUT IS 64x64\n",
    "IMG_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Resize((IMG_DIM, IMG_DIM)),\n",
    "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transformations = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Resize((IMG_DIM, IMG_DIM)),\n",
    "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the experiment we may want to load all annotated images or just the ones that are annotated by experts.\n",
    "\n",
    "Here we create a function to load all the images that are annotated (not only the ones by experts).\n",
    "The created function can than be used to load the data loaders with a specific batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train, test split ratio is set to 0.8 by default.\n",
    "# Meaning 80% of the data is used for training and 20% for testing.\n",
    "load_annotated_data = make_annotated_loader(train_transformations, test_transformations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we determine the device we want to train on. \n",
    "If a GPU is available, we use it, otherwise we fall back to the CPU. \n",
    "We also set the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the model we want to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hubmap.models.fully_convolutional_transformer import FCT\n",
    "from hubmap.models.fully_convolutional_transformer import init_weights\n",
    "\n",
    "model = FCT()\n",
    "init_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (512x256x1). Calculated output size: (512x128x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Quick test for random input.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m out \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39;49mzeros(size\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, IMG_DIM, IMG_DIM, \u001b[39m3\u001b[39;49m))\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mimshow(out\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/miniconda3/envs/J/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/lmu/HuBMAP/hubmap/models/fully_convolutional_transformer.py:359\u001b[0m, in \u001b[0;36mFCT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Multi-scale input\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     scale_img_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_img(x)\n\u001b[0;32m--> 359\u001b[0m     scale_img_3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_img(scale_img_2)\n\u001b[1;32m    360\u001b[0m     scale_img_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_img(scale_img_3)\n\u001b[1;32m    362\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/J/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/J/lib/python3.11/site-packages/torch/nn/modules/pooling.py:639\u001b[0m, in \u001b[0;36mAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mavg_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    640\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcount_include_pad, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdivisor_override)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (512x256x1). Calculated output size: (512x128x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "# Quick test for random input.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "out = model(torch.zeros(size=(1, IMG_DIM, IMG_DIM, 3)).to(device))\n",
    "plt.imshow(out.squeeze().permute(1, 2, 0).detach().cpu())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the other modules needed for training, such as the loss measure, and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = BCEDiceLoss()\n",
    "# WE ARE ONLY INTERESTED IN THE IoU OF THE BLOOD VESSEL CLASS FOR NOW.\n",
    "benchmark = IoU(class_index=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the trainer and start training. The trainer is responsible for running the training loop, saving checkpoints, and logging metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "train_loader, test_loader = load_annotated_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition we want to have a dynamic adjustment of the learning rate.\n",
    "lr_scheduler = LRScheduler(optimizer, patience=20, min_lr=1e-6, factor=0.8)\n",
    "# We will ignore the early stopping in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train(\n",
    "    num_epochs=2,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    benchmark=benchmark,\n",
    "    checkpoint_name=\"dpt_trials\",\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the results.\n",
    "(*this needs improvements + better and more visualizations for the final paper*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from hubmap.visualization import visualize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_path = Path().cwd() / \"figures\"\n",
    "os.makedirs(figures_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fig, benchmark_fig = visualize_result(result)\n",
    "loss_fig.savefig(Path(figures_path, \"dpt_loss.png\"))\n",
    "benchmark_fig.savefig(Path(figures_path, \"dpt_benchmark.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "J",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a58ea3f06b7c035f03696c8d262430e329e4f861a31188ab1909a2b4ddd27d3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
